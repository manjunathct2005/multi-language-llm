# üìò Part 8: Deep Learning (Highly Detailed)

Deep Learning (DL) is a subfield of Machine Learning (ML) that uses **artificial neural networks (ANNs)** with many layers to learn from vast amounts of data. Inspired by the human brain, deep learning has powered advancements in **computer vision**, **speech recognition**, **natural language understanding**, and more.

---

## üîç 1. What is a Neural Network?

A **neural network** is made up of layers of **neurons (also called nodes)** that take input, process it using weights and biases, apply an **activation function**, and pass the output to the next layer.

### üß† Key Components:

* **Input Layer**: Accepts feature inputs
* **Hidden Layers**: Perform calculations
* **Output Layer**: Returns prediction/class
* **Weights**: Determine importance of input
* **Biases**: Added constants
* **Activation Function**: Introduces non-linearity

### üßæ Example: Building a Simple ANN with Keras

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(10,)))
model.add(Dense(1, activation='sigmoid'))
```

**Explanation:**

* `Dense(64)`: Fully connected layer with 64 neurons.
* `activation='relu'`: Applies ReLU function.
* `sigmoid`: For binary classification (e.g., spam vs not spam).

---

## ‚öôÔ∏è 2. Activation Functions

| Function | Full Form             | Purpose                          |
| -------- | --------------------- | -------------------------------- |
| ReLU     | Rectified Linear Unit | Keeps only positive values       |
| Sigmoid  | -                     | Maps values to range (0,1)       |
| Tanh     | Hyperbolic Tangent    | Range (-1, 1), centered at zero  |
| Softmax  | -                     | Converts output to probabilities |

---

## üìâ 3. Loss Functions (What we try to minimize)

| Loss Type                | Use Case                   |
| ------------------------ | -------------------------- |
| Binary Crossentropy      | Binary classification      |
| Categorical Crossentropy | Multi-class classification |
| Mean Squared Error (MSE) | Regression                 |

```python
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

---

## ‚öôÔ∏è 4. Optimizers (How model learns)

| Optimizer | Full Form                   | Purpose                          |
| --------- | --------------------------- | -------------------------------- |
| SGD       | Stochastic Gradient Descent | Basic optimizer, slow but simple |
| Adam      | Adaptive Moment Estimation  | Combines RMSProp and Momentum    |

---

## üìä 5. Training & Validation

```python
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
```

**Explanation:**

* `epochs`: Number of full training cycles
* `batch_size`: Number of samples per update
* `validation_split`: Holdout data to evaluate generalization

---

## üñºÔ∏è 6. Convolutional Neural Networks (CNNs)

CNNs are specialized for **image processing**. They apply filters to input images to capture **spatial patterns**.

### üîß Example:

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential([
  Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 3)),
  MaxPooling2D(2, 2),
  Flatten(),
  Dense(128, activation='relu'),
  Dense(1, activation='sigmoid')
])
```

### üß† Key Terms:

* **Conv2D**: Applies filters to detect edges/patterns
* **MaxPooling2D**: Downsamples the image
* **Flatten**: Converts to 1D
* **Dense**: Final fully connected classifier

---

## ‚è≥ 7. Recurrent Neural Networks (RNNs) and LSTM

Used for **sequence data** like text, audio, time series.

| Layer | Full Form                | Purpose                            |
| ----- | ------------------------ | ---------------------------------- |
| RNN   | Recurrent Neural Network | Remembers past data                |
| LSTM  | Long Short-Term Memory   | Overcomes short-term memory issues |

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense

model = Sequential([
  Embedding(input_dim=10000, output_dim=128),
  LSTM(64),
  Dense(1, activation='sigmoid')
])
```

---

# üìò Part 9: Natural Language Processing (NLP)

**NLP (Natural Language Processing)** deals with making machines understand and generate human language. It powers applications like:

* Chatbots
* Translation systems
* Text classification
* Speech recognition

---

## ‚úÇÔ∏è 1. Text Preprocessing

```python
import re
text = "Text@Example with PUNCTUATION!"
text = re.sub(r"[^a-zA-Z0-9 ]", "", text.lower())
```

**Explanation:**

* Removes punctuation
* Converts text to lowercase

---

## üî° 2. Tokenization

| Tool                | Purpose                |
| ------------------- | ---------------------- |
| `word_tokenize`     | Splits text into words |
| `sentence_tokenize` | Splits into sentences  |

```python
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
tokens = word_tokenize("Data science is great")
```

---

## üõë 3. Stopwords

```python
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
filtered = [w for w in tokens if w not in stop_words]
```

**Stopwords**: Common words like 'is', 'the', 'in', which do not add meaning.

---

## üî¢ 4. Text Vectorization (TF-IDF)

| Term | Full Form                  | Purpose                     |
| ---- | -------------------------- | --------------------------- |
| TF   | Term Frequency             | Frequency of term in doc    |
| IDF  | Inverse Document Frequency | Measures uniqueness of term |

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer()
X = vec.fit_transform(["data science", "machine learning science"])
```

---

## ü§ñ 5. Transformers & BERT

| Model | Full Form                                               | Purpose                |
| ----- | ------------------------------------------------------- | ---------------------- |
| BERT  | Bidirectional Encoder Representations from Transformers | Language understanding |
| GPT   | Generative Pretrained Transformer                       | Text generation        |

```python
from transformers import pipeline
pipe = pipeline("sentiment-analysis")
pipe("I love NLP!")
```

---

# üìò Part 10: AI Fundamentals

**AI (Artificial Intelligence)** is the broader field where machines mimic **cognitive functions** like learning, problem-solving, and reasoning.

---

## üß† 1. Types of AI

| Type | Description                     |
| ---- | ------------------------------- |
| ANI  | Artificial Narrow Intelligence  |
| AGI  | Artificial General Intelligence |
| ASI  | Artificial Super Intelligence   |

---

## üïπÔ∏è 2. Applications of AI

* Computer Vision
* NLP
* Robotics
* Gaming
* Predictive Maintenance

---

## ü§ñ 3. Expert Systems

* Rule-based systems that use IF-THEN logic
* Used in medical diagnosis, fraud detection

---

## üß≠ 4. Branches of AI

* ML (Machine Learning)
* DL (Deep Learning)
* NLP
* CV (Computer Vision)
* RL (Reinforcement Learning)

---

# üìò Part 11: Model Deployment & MLOps

**MLOps** (Machine Learning Operations) is a set of practices for deploying and maintaining ML models in production.

---

## üöÄ 1. Model Deployment with Flask

```python
from flask import Flask, request, jsonify
import joblib

model = joblib.load("model.pkl")
app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
  data = request.get_json()
  prediction = model.predict([list(data.values())])
  return jsonify({'prediction': int(prediction[0])})
```

---

## üê≥ 2. Docker for Containerization

```dockerfile
FROM python:3.9
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
CMD ["python", "app.py"]
```

**Docker** packages your app and its dependencies into a single image.

---

## üîÅ 3. CI/CD Pipelines

| Term | Full Form              |
| ---- | ---------------------- |
| CI   | Continuous Integration |
| CD   | Continuous Deployment  |

Tools: GitHub Actions, Jenkins, GitLab CI

---

# üìò Part 12: Projects & Case Studies

### ‚úÖ 1. Soil Fertility Prediction

* **Model**: Random Forest
* **Inputs**: Nitrogen, Phosphorus, Potassium, pH
* **Output**: Fertility category (High, Medium, Low)

### ‚úÖ 2. Cancer Detection from CT Scans

* **Model**: Convolutional Neural Network (CNN)
* **Input**: CT Images
* **Output**: Tumor (Yes/No)

### ‚úÖ 3. Multilingual Chatbot

* **Tools**: Hugging Face Transformers, Streamlit
* **Languages**: Hindi, Telugu, English
* **Features**: Speech-to-text, text classification, response generation

---
