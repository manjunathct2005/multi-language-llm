
---

## 📘 Part 3: Mathematics for Data Science (Highly Detailed)

Mathematics is the **foundation of Data Science and Machine Learning**. It allows you to understand **how algorithms work under the hood**, debug errors, improve models, and design new ones.

We’ll cover:

1. Linear Algebra
2. Probability & Statistics
3. Calculus
4. Optimization
5. Information Theory
6. Real-world Applications in ML

---

### 🔹 1. Linear Algebra

Linear Algebra is the study of **vectors, matrices, and linear transformations**. In ML and AI, data is almost always represented as matrices, and model operations are matrix operations.

#### ✅ Key Concepts:

* **Scalars**: Single number (e.g., 4)
* **Vectors**: 1D array (e.g., \[1, 2, 3])
* **Matrices**: 2D array (e.g., \[\[1, 2], \[3, 4]])
* **Tensors**: Higher-dimensional arrays

#### ✅ Matrix Operations:

* **Addition/Subtraction**: Element-wise
* **Dot Product**: Combines two vectors/matrices
* **Transpose**: Flip matrix over its diagonal
* **Inverse**: Used to solve matrix equations

#### 🔧 NumPy Example:

```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Matrix multiplication
C = np.dot(A, B)
# Transpose
A_T = A.T
```

#### 📌 Applications:

* Feature representations
* PCA (dimensionality reduction)
* Neural Network forward pass

---

### 🔹 2. Probability & Statistics

Probability helps us **model uncertainty**, and statistics helps us **summarize and interpret data**.

#### ✅ Key Probability Concepts:

* **Event**: Outcome of a process (e.g., flipping heads)
* **Probability (P)**: Likelihood (0 ≤ P ≤ 1)
* **Conditional Probability**: P(A|B) = P(A ∩ B) / P(B)
* **Bayes Theorem**:

  ```
  P(A|B) = P(B|A) * P(A) / P(B)
  ```

#### ✅ Distributions:

* **Bernoulli**: Yes/No (binary)
* **Binomial**: Number of successes in n trials
* **Normal (Gaussian)**: Bell-shaped curve (used in linear models)

#### ✅ Key Statistical Measures:

* **Mean, Median, Mode**
* **Variance & Standard Deviation**
* **Skewness & Kurtosis**
* **Correlation vs Covariance**

#### 🔧 Example:

```python
import scipy.stats as stats
import numpy as np

# Normal distribution
mean = 0
std = 1
x = np.linspace(-3, 3, 100)
pdf = stats.norm.pdf(x, mean, std)
```

#### 📌 Applications:

* Model evaluation (confidence intervals)
* Data exploration
* Naive Bayes classifier

---

### 🔹 3. Calculus

Calculus, especially **differentiation**, is essential for optimizing models.

#### ✅ Derivatives:

* Rate of change of a function
* `f'(x)` means derivative of function `f` at point `x`

#### ✅ Chain Rule:

Used in **neural networks** during backpropagation.

#### ✅ Gradient:

Vector of partial derivatives (used in optimization).

#### 🔧 Example:

```python
import sympy as sp
x = sp.symbols('x')
f = x**3 + 2*x**2
derivative = sp.diff(f, x)
print(derivative)  # Output: 3x^2 + 4x
```

#### 📌 Applications:

* Backpropagation in Deep Learning
* Cost function minimization
* Gradient Descent

---

### 🔹 4. Optimization

Optimization is about **minimizing or maximizing a function**, like reducing model error (loss).

#### ✅ Types:

* **Gradient Descent**: Iterative optimization
* **Stochastic Gradient Descent (SGD)**
* **Momentum**, **RMSProp**, **Adam** (used in deep learning)
* **Convex vs Non-convex functions**

#### 📌 Applications:

* Training models
* Tuning hyperparameters

---

### 🔹 5. Information Theory

Used in **NLP**, **feature selection**, and **decision trees**.

#### ✅ Key Concepts:

* **Entropy**: Uncertainty in a dataset
* **Information Gain**: Used in Decision Trees
* **KL Divergence**: Difference between two distributions

---

### 🔹 6. Practical Applications in Machine Learning

| Math Area          | Used In                               |
| ------------------ | ------------------------------------- |
| Linear Algebra     | Neural Nets, PCA                      |
| Probability        | Naive Bayes, Hidden Markov Models     |
| Statistics         | Feature Engineering, Model Evaluation |
| Calculus           | Backpropagation in Neural Nets        |
| Optimization       | Gradient Descent, Model Training      |
| Information Theory | Decision Trees, NLP Tokenization      |

---

### 🧠 Summary

Mathematics is NOT optional in data science — it powers everything:

* Understanding of models
* Interpretation of results
* Customization of algorithms
---

## 📘 Part 4: Data Collection and Cleaning (Fully Explained)

---

### 🔹 1. Data Collection

Data collection is the **first step** in the data science pipeline. You gather raw data from **files, databases, APIs, web pages, or sensors** to analyze it or build models.

---

#### 📁 A. Reading Data from Files

##### Example 1: Reading a CSV file

```python
import pandas as pd
df = pd.read_csv("customers.csv")
```

**Explanation:**

* `pandas` is a Python library used for working with structured data.
* `read_csv()` loads the contents of a `.csv` file into a **DataFrame** (a table-like structure).
* `"customers.csv"` is the filename; it should be in your working directory.

---

##### Example 2: Reading an Excel file

```python
df = pd.read_excel("sales_data.xlsx", sheet_name="2023")
```

**Explanation:**

* `read_excel()` reads an Excel file and allows specifying a sheet name (like `"2023"`).
* Useful when working with reports or financial data.

---

##### Example 3: Reading a JSON file

```python
df = pd.read_json("products.json")
```

**Explanation:**

* JSON is a common data format, especially in APIs.
* This loads the `.json` into a DataFrame automatically if it's structured correctly.

---

#### 🗄️ B. Reading Data from SQL Databases

```python
import sqlite3
conn = sqlite3.connect("data.db")
query = "SELECT * FROM sales"
df = pd.read_sql(query, conn)
```

**Explanation:**

* `sqlite3.connect()` connects to a SQLite database file.
* `pd.read_sql()` runs the SQL query (`SELECT *`) and fetches the results into a DataFrame.
* Used when data is stored in relational databases (MySQL, SQLite, PostgreSQL, etc.).

---

#### 🌐 C. Collecting Data from APIs

```python
import requests

url = "https://api.coindesk.com/v1/bpi/currentprice.json"
response = requests.get(url)
data = response.json()
```

**Explanation:**

* `requests.get(url)` sends an HTTP GET request to retrieve data from the web.
* `.json()` converts the response from JSON format to a Python dictionary.
* Used to collect live or dynamic data like stock prices, weather, etc.

---

#### 🕸️ D. Web Scraping with BeautifulSoup

```python
from bs4 import BeautifulSoup
html = requests.get("https://books.toscrape.com").text
soup = BeautifulSoup(html, "html.parser")
titles = soup.find_all("h3")
```

**Explanation:**

* `BeautifulSoup` parses HTML pages.
* `.find_all("h3")` grabs all `<h3>` tags (typically headings like book titles).
* Useful when no API is available, but data is shown on web pages.

---

### 🔹 2. Data Cleaning

Now that data is collected, we need to **clean** it — removing or correcting inaccuracies, missing values, or irrelevant details.

---

#### 🧼 A. Identifying Missing Values

```python
df.isnull().sum()
```

**Explanation:**

* `isnull()` flags missing values as `True`.
* `.sum()` counts total nulls per column.
* Helps identify where data is incomplete.

---

#### Filling Missing Values

```python
df['Age'].fillna(df['Age'].mean(), inplace=True)
```

**Explanation:**

* Replaces missing values in `'Age'` with the average age.
* `inplace=True` updates the original DataFrame.
* Keeps numeric features usable by ML algorithms.

---

#### Replacing with a constant

```python
df['Gender'].fillna("Unknown", inplace=True)
```

**Explanation:**

* Categorical features are sometimes best filled with placeholders.

---

#### 🧼 B. Removing Duplicates

```python
df.drop_duplicates(inplace=True)
```

**Explanation:**

* Detects and removes duplicate rows.
* Prevents repeated data from skewing analysis or predictions.

---

#### 🧼 C. Detecting and Removing Outliers (Using IQR)

```python
Q1 = df['Income'].quantile(0.25)
Q3 = df['Income'].quantile(0.75)
IQR = Q3 - Q1

df = df[(df['Income'] >= Q1 - 1.5 * IQR) & (df['Income'] <= Q3 + 1.5 * IQR)]
```

**Explanation:**

* `IQR` (Interquartile Range) is a method to detect outliers.
* This code filters out values outside 1.5 \* IQR below Q1 or above Q3.

---

#### 🧼 D. Data Type Conversion

```python
df['Date'] = pd.to_datetime(df['Date'])
```

**Explanation:**

* Converts text date into Python datetime format.
* Allows extracting year, month, day, etc.

```python
df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')
```

**Explanation:**

* Converts strings to numbers.
* Invalid entries will be replaced with `NaN` (useful before cleaning).

---

#### 🧼 E. Renaming Columns for Consistency

```python
df.columns = [col.lower().strip().replace(" ", "_") for col in df.columns]
```

**Explanation:**

* Lowercases all column names.
* Replaces spaces with underscores (good for programming).
* Ensures consistency across scripts and functions.

---

#### 🧼 F. Encoding Categorical Variables

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Gender'] = le.fit_transform(df['Gender'])
```

**Explanation:**

* Converts text labels into numbers (`Male` → 1, `Female` → 0).
* Works for ordinal or binary categories.

---

```python
df = pd.get_dummies(df, columns=['City'], drop_first=True)
```

**Explanation:**

* One-hot encodes a column into multiple binary columns (`City_NewYork`, `City_LA`).
* `drop_first=True` avoids multicollinearity.

---

#### 🧼 G. Feature Scaling

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df[['Age', 'Income']] = scaler.fit_transform(df[['Age', 'Income']])
```

**Explanation:**

* `StandardScaler` normalizes data to have mean = 0, std = 1.
* Needed for models sensitive to feature scale (like SVM, KNN).

---

### 🔹 3. Feature Engineering

```python
df['year'] = df['date'].dt.year
df['is_weekend'] = df['date'].dt.dayofweek > 4
```

**Explanation:**

* Extracts year from a date.
* Creates a boolean column indicating whether the date falls on a weekend.

```python
df['age_group'] = pd.cut(df['age'], bins=[0, 18, 30, 50, 100],
                         labels=["Teen", "Young", "Adult", "Senior"])
```

**Explanation:**

* Groups numerical age into categorical bins.
* Useful for demographic analysis or categorical model inputs.

---

### 🔹 4. Data Profiling Tools

```bash
pip install pandas-profiling
```

```python
from pandas_profiling import ProfileReport
report = ProfileReport(df, title="Dataset Report")
report.to_file("report.html")
```

**Explanation:**

* Automatically generates a detailed report of your data: distributions, correlations, missing data, etc.
* Saves time and gives insights before modeling.

---

### 🧠 Summary

| Step                | Goal                                               |
| ------------------- | -------------------------------------------------- |
| Data Collection     | Bring data from files, web, APIs, or databases     |
| Data Cleaning       | Make the data usable — clean, complete, consistent |
| Feature Engineering | Add new meaning — transform raw features           |
| Profiling           | Automated understanding of data                    |

Every successful model starts with high-quality data.

---
